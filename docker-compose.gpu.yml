# HyperMatrix v2026.01 + Ollama Stack (GPU + API Support)
# Docker Compose with GPU acceleration and cloud API fallback
#
# MODOS DE USO:
#   1. GPU Local:  AI_PROVIDER=ollama + GPU habilitada
#   2. CPU Local:  AI_PROVIDER=ollama + sin GPU (mÃ¡s lento)
#   3. Cloud API:  AI_PROVIDER=openai|anthropic|groq (requiere API key)
#
# Para usar este archivo:
#   docker-compose -f docker-compose.gpu.yml up -d

services:
  # ===========================================================================
  # HyperMatrix - Code Analysis Dashboard
  # ===========================================================================
  hypermatrix:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: hypermatrix
    ports:
      - "26020:26020"
    environment:
      # === Configuracion basica ===
      - HYPERMATRIX_PORT=26020
      - HYPERMATRIX_HOST=0.0.0.0
      - DATA_DIR=/app/data

      # === Proveedor de IA (ollama | openai | anthropic | groq) ===
      - AI_PROVIDER=${AI_PROVIDER:-ollama}

      # === Ollama Local ===
      - OLLAMA_HOST=ollama:11434
      - OLLAMA_MODEL=${OLLAMA_MODEL:-openhermes:latest}

      # === APIs Cloud (solo si AI_PROVIDER != ollama) ===
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4o-mini}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - ANTHROPIC_MODEL=${ANTHROPIC_MODEL:-claude-3-haiku-20240307}
      - GROQ_API_KEY=${GROQ_API_KEY:-}
      - GROQ_MODEL=${GROQ_MODEL:-llama-3.1-8b-instant}
    volumes:
      - hypermatrix_data:/app/data
      - ${PROJECTS_PATH:-./projects}:/projects:ro
      - hypermatrix_logs:/app/logs
    deploy:
      resources:
        limits:
          cpus: '${HYPERMATRIX_CPUS:-2}'
          memory: ${HYPERMATRIX_MEMORY:-2G}
    depends_on:
      ollama:
        condition: service_healthy
    networks:
      - hypermatrix-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:26020/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # ===========================================================================
  # Ollama - Local LLM Server (GPU Accelerated)
  # ===========================================================================
  ollama:
    image: ollama/ollama:latest
    container_name: hypermatrix-ollama
    ports:
      - "11435:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_NUM_PARALLEL=${OLLAMA_NUM_PARALLEL:-2}
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ollama_models:/root/.ollama
    deploy:
      resources:
        limits:
          cpus: '${OLLAMA_CPUS:-6}'
          memory: ${OLLAMA_MEMORY:-12G}
        reservations:
          # === GPU NVIDIA (comentar si no tienes GPU) ===
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - hypermatrix-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # ===========================================================================
  # Ollama Model Initializer (downloads model on first run)
  # ===========================================================================
  ollama-init:
    image: curlimages/curl:latest
    container_name: hypermatrix-ollama-init
    depends_on:
      ollama:
        condition: service_healthy
    networks:
      - hypermatrix-network
    entrypoint: >
      sh -c "
        echo 'Downloading model: ${OLLAMA_MODEL:-openhermes:latest}...' &&
        curl -X POST http://ollama:11434/api/pull -d '{\"name\": \"${OLLAMA_MODEL:-openhermes:latest}\"}' &&
        echo 'Model ready!'
      "
    restart: "no"

networks:
  hypermatrix-network:
    driver: bridge

volumes:
  hypermatrix_data:
  hypermatrix_logs:
  ollama_models:
